---
published: true
layout: posts
title: '[DL4C] Ch5. Image Classification'
categories: 
  - dl4coders
---

---

Answer reference:

- [Fastbook Chapter 5 questionnaire solutions (wiki)](https://forums.fast.ai/t/fastbook-chapter-5-questionnaire-solutions-wiki/69301)
    

## CheckPoint

Deep Dive into the DL

- What is the architecture of a computer vision model, an NLP model, a tabular model, and so on?
- How do you create an architecture that matches the needs of your particular domain?
- How do you get the best possible results from the training process?
- How do you make things faster?
- What do you have to change as your datasets change?

Deep Learning Puzzle

- different types of layers
- regularization methods
- optimizers
- how to put layers together into architectures
- labeling techniques
    - and much moreâ€¦

---

## Questionnaire

1. Why do we first resize to a large size on the CPU, and then to a smaller size on the GPU? (x)
    - Presize Strategy in fastai
    - ë³´í†µì€ GPUì—ì„œ ìˆ˜í–‰ë˜ë‚˜, ê³„ì‚°â†’ë³´ê°„ì˜ ë°˜ë³µìœ¼ë¡œ ë°ì´í„° ì†ì‹¤ ë° í’ˆì§ˆ ì €í•˜
        - augmentation â†’ ì´ë¯¸ì§€ì˜ ìµœëŒ€ í¬ê¸°ì—ì„œ ìˆ˜í–‰ë¨ (CPU)
        - RandomSizeCrop â†’ ìµœì¢… ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆì—ì„œ ì¶•ì†Œë˜ì–´ ìˆ˜í–‰ë¨ (GPU)

1. If you are not familiar with regular expressions, find a regular expression tutorial, and some problem sets, and complete them. Have a look on the book's website for suggestions.
2. What are the two ways in which data is most commonly provided, for most deep learning datasets?
    - ë°ì´í„° ëª©ë¡ìœ¼ë¡œì„œì˜ íŒŒì¼ ë¬¶ìŒ
        - ë¬¸ì„œ, ì´ë¯¸ì§€ ë“±
        - í´ë” / íŒŒì¼ ì´ë¦„ìœ¼ë¡œ ëª©ë¡ì— ëŒ€í•œ ì •ë³´ ì œê³µ
    - í…Œì´ë¸” ë°ì´í„°
        - ê° í–‰ì€ í•˜ë‚˜ì˜ ë°ì´í„°
            - ë°ì´í„° â†” í…Œì´ë¸”/ë‹¤ë¥¸ í˜•íƒœ ë°ì´í„° ì‚¬ì´ ê´€ê³„ ì œê³µ
3. Look up the documentation for `L` and try using a few of the new methods that it adds.
4. Look up the documentation for the Python `pathlib` module and try using a few methods of the `Path` class.

1. Give two examples of ways that image transformations can degrade the quality of the data.
    - rotation - empty zone - interpolation â†’ bad quality
    - zoom in - interpolation â†’ bad quality

1. What method does fastai provide to view the data in a `DataLoaders`?
    - DataLoaders.show_batch(nrows=<>, ncols=<>)
2. What method does fastai provide to help you debug a `DataBlock`?
    - DataBlock.sumarry(<path>)

1. Should you hold off on training a model until you have thoroughly cleaned your data?
    
    <aside>
    ğŸ’¡ Nope! We should create a baseline model as soon as possible.
    
    </aside>
    
2. What are the two pieces that are combined into cross-entropy loss in PyTorch?
    - activation_fuction : softmax
    - negative_log_likelihood
        - (log_softmax + nll_loss) = cross_entrophy
3. What are the two properties of activations that softmax ensures? Why is this important?
    - ì–´ë–¤ ê°’ë„ 0, 1 ì‚¬ì´ì—
    - ëª¨ë“  ê°’  ë”í•´ì„œ 1
        - í™•ë¥ ë¶„í¬ / actì˜ ì°¨ì´ë¥¼ ì§€ìˆ˜ë¥¼ í†µí•´ amplifyí•´ ì¤Œ
4. When might you want your activations to not have these two properties? (x)
    - multi-label classifiction problem
5. Calculate the `exp` and `softmax` columns of <<bear_softmax>> yourself (i.e., in a spreadsheet, with a calculator, or in a notebook).
    
    ```python
    acts
    
    def softmax(x): return np.exp(x) / exp(x).sum(dim=1, keepdim=True) 
    
    def neg_log_likelihood(x): return -np.log(tensor(x))
    
    def cross_entrophy_loss(x): return neg_log_likelihood(softmax(x)).sum()
        
    ```
    

1. Why can't we use `torch.where` to create a loss function for datasets where our label can have more than two categories?
    - it uses as list comprehension with if-else statements.
    - binary case only
2. What is the value of log(-2)? Why?
    - not defined. ë¡œê·¸ëŠ” ìŒìˆ˜ë¥¼ ì§„ìˆ˜ë¡œ ì·¨í•  ìˆ˜ ì—†ë‹¤.
    - yëŠ” ì§€ìˆ˜ ê³„ì‚°ì„ í†µí•´ ë‚˜ì˜¨ ê°’. ìŒìˆ˜ê°€ ë¶ˆê°€ëŠ¥.
    
    $$
    e^x = y\\
    x = log{y}
    
    $$
    

---

## Improve Our Model

### 1. Learning Rate Finder

- ì•„ì£¼ ì‘ì€ í•™ìŠµë¥ ì—ì„œ ì‹œì‘
- 1ê°œì˜ mini-batch ëŒë ¤ì„œ loss ê³„ì‚°
- í•™ìŠµë¥  ì¼ì • ë¹„ìœ¨ë¡œ ì¦ê°€ (ex.ë§¤ë²ˆ *2)
- ìœ„ ê³¼ì •ì„ ê³„ì† ë°˜ë³µ
    - lossê°€ ë‚˜ì•„ì§€ì§€ ì•Šê³  ë” ë‚˜ë¹ ì§ˆ ë•Œ ê¹Œì§€!
1. What are two good rules of thumb for picking a learning rate from the learning rate finder?
    - minimum loss ë‹¬ì„±ëœ ë¶€ë¶„ì—ì„œ í•œ ë‹¨ê³„ ì¤„ì´ê¸°
        - min / 10
    - lossê°€ í™•ì‹¤íˆ ì¤„ì–´ë“¤ê¸° ì‹œì‘í•˜ëŠ” ë§ˆì§€ë§‰ ë¶€ë¶„

### 2. Unfreezing and Transfer Learning

#### Transfer Learning

- ê¸°ì¡´ì˜ final linear layerë¥¼ ì œê±°
- í˜„ì¬ ìƒí™©ì— ë§ëŠ” outputìœ¼ë¡œ ë§ì¶˜ layer ì¶”ê°€
    
    â†’ ì „ì²´ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•˜ëŠ” ê²ƒì´ ì•„ë‹˜!
    

> ğŸ’¡ ë§ˆì§€ë§‰ layerì˜ random weightsë¥¼ â€˜ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜â€™ë¥¼ ì†ìƒì‹œí‚¤ì§€ ì•Šìœ¼ë©´ì„œ ì–´ë–»ê²Œ í•™ìŠµ ì‹œí‚¬ ê²ƒì¸ê°€?!

- optimzer ë¥¼ ì¡°ì •í•´ ë§ˆì§€ë§‰ layerë§Œ ê°€ì¤‘ì¹˜ ì¡°ì •í•˜ë„ë¡ ì„¤ì • ê°€ëŠ¥ (ë‚˜ë¨¸ì§€ ë¶€ë¶„ì€ ê³ ì •)
    - `freezing` pretrained layers

1. What two steps does the `fine_tune` method do? (freeze, unfreeze)
    - ëª¨ë“  ë‹¤ë¥¸ layersë“¤ì„ freezingí•œ ì±„ë¡œ ë§ˆì§€ë§‰ layerë§Œ 1 epoch í•™ìŠµì„ ìˆ˜í–‰
    - ëª¨ë“  layerë¥¼ unfreezeí•˜ê³  (n) epochë§Œí¼ í•™ìŠµì‹œí‚´
2. In Jupyter Notebook, how do you get the source code for a method or function? â†’ method??

### 3. Discriminative Learning Rate

- pretrained weights have been trained for hundreds of epochs, on millions of images
    - let the layer layers fine-tune more quickly than earlier layers
- nnì˜ ì´ˆê¸° layerëŠ” ë‚®ì€ í•™ìŠµë¥ ë¡œ, ë‚˜ì¤‘ì˜ layerëŠ” ë†’ì€ í•™ìŠµë¥ ë¡œ

>ğŸ’¡ lossì˜ ë³€í™”ë„ ì¤‘ìš”í•˜ì§€ë§Œ (learn.recorder.plot_loss)
> ê°€ì¥ ì¤‘ìš”í•œê±´ metrics! not lossâ€¦

1. What are discriminative learning rates?
    - lower layer, higher layer different learning rate
2. How is a Python `slice` object interpreted when passed as a learning rate to fastai?
    - first : the earliest layers
    - second : the final layer
        - in the middle â†’ multiplicatively equidistant throughout the range

### 4. Selecting the Number of Epochs

- ì²˜ìŒì—ëŠ” ê¸°ë‹¤ë¦´ ìˆ˜ ìˆëŠ” ì ë‹¹í•œ ì‹œê°„ì˜ epochë¥¼ ê³¨ë¼ì„œ ìˆ˜í–‰í•œë‹¤
    - In practice, í•­ìƒ ì¤‘ìš”í•œê±´ metrics!!!
        - lossê°€ ë‚˜ë¹ ì§€ëŠ”ê±°ì— ë„ˆë¬´ ì‹ ê²½ì“°ì§€ ë§ê¸°
    - ì´ˆë°˜ì—ëŠ” ê³¼ì í•©ìœ¼ë¡œ loss ë‚˜ë¹ ì§ˆ ìˆ˜ ìˆìŒ
        - í›„ë°˜ì— ë°ì´í„°ë¥¼ ì˜ëª» ê¸°ì–µí•´ì„œ ë°œìƒí•˜ëŠ” loss ì €í•˜ë§Œ ì‹ ê²½ì“°ë©´ ë¨

1. Why is early stopping a poor choice when using 1cycle training? â†’ (x)
- learning rateì´ ë” ë‚®ê²Œ ë˜ê¸° ê¹Œì§€ ì¶©ë¶„í•œ ì‹œê°„ì´ ë˜ê¸° ì „ì— ì¢…ë£Œë˜ë²„ë¦´ ìœ„í—˜
    - ëª¨ë¸ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆëŠ” ë° ì¢…ë£Œë˜ì–´ ë²„ë¦¼

#### í•´ê²°
##### Retrain
- ë‹¤ì‹œ ë°”ë‹¥ë¶€í„° ëª¨ë¸ì„ ì¬í›ˆë ¨í•œë‹¤
    - ì§ì „ì˜ ìµœê³  ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆì„ ë•Œë¥¼ ê¸°ì¤€ìœ¼ë¡œ epochë¥¼ ì„ íƒí•œë‹¤

### 5. Deeper Architectures

#### ì¥ë‹¨ì  ë¹„êµ
- ë” í°(ê¹Šì€) ëª¨ë¸ â†’ ë” ë‚˜ì€ í•™ìŠµ ì†ì‹¤ ì œê³µ
    - ê³¼ì í•©ì˜ ìœ„í—˜ì„±ë„ ì»¤ì§ (íŒŒë¼ë¯¸í„°ê°€ ë§ì•„ì„œ)
    - more layers and parameters = larger model = the capacity of model
- ë” í°(ê¹Šì€) ëª¨ë¸ â†’ ë°ì´í„°ì˜ ìˆ¨ê²¨ì§„ ê´€ê³„ë„ ì˜ ì°¾ìŒ
    - ê°œë³„ ì´ë¯¸ì§€ì˜ íŠ¹ìˆ˜í•œ ë¶€ë¶„ë„ ì˜ ê¸°ì–µí•¨

ê¹Šì€ í•™ìŠµ â†’ GPU ë©”ëª¨ë¦¬ ë§ì´ ì¡ì•„ë¨¹ìŒ â†’ ë»—ì„ìˆ˜ë„â€¦

#### í•´ê²°
##### mixed-precision training

- less-precise number (half-precision floating point : fp16) ì‚¬ìš©

[fastai - Mixed precision training](https://docs.fast.ai/callback.fp16.html)

1. What is the difference between `resnet50` and `resnet101`?
    - how deep nn is (num of layers)
2. What does `to_fp16` do?
    - float point drop
    - í…ì„œì˜ ì •ë³´ëŸ‰ ì¤„ì—¬ì„œ ì—°ì‚° ì†ë„ ë¹ ë¥´ê²Œ


> ğŸ’¡ í° ëª¨ë¸ì´ í•­ìƒ ë” ì¢‹ì€ê±´ ì•„ë‹ˆë‹¤! (íŠ¹ì • ë¶€ë¶„ì—ì„œëŠ” ë”ìš±)
> ì‘ì€ ëª¨ë¸ì—ì„œ ì‹œì‘í•´ì„œ Scaling-up í•´ê°€ê¸°


---

## Further Research - Why?

Thatâ€™s because the gradient of cross-entropy loss is proportional to the
difference between the activation and the target, so SGD always gets a
nicely scaled step for the weights.

An interesting feature about cross-entropy loss appears when we consider its gradient. The gradient of cross_entropy(a,b) is softmax(a)-b. Since softmax(a) is the final activation of the model, that means that the gradient is proportional to the difference between the prediction and the target. This is the same as mean squared error in regression (assuming thereâ€™s no final activation function such as that added by y_range), since the gradient of (a-b)**2 is 2*(a-b). Because the gradient is linear, we wonâ€™t see sudden jumps or exponential increases in gradients, which should lead to smoother training of models.